{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--Cvru1cgwyP"
   },
   "source": [
    "# **Miniproject 2**\n",
    "## **~Large~ Small Language Model**\n",
    "\n",
    "### **Objective**\n",
    "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
    "\n",
    "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_rT3xwrhieb"
   },
   "source": [
    "### **Dataset**:\n",
    "\n",
    "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
    "\n",
    "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "\n",
    "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
    "\n",
    "An interface for the dataset class that takes care of tokenization is provided below.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV7OAXGRhf_V"
   },
   "source": [
    "### **Requirements**\n",
    "\n",
    "#### **Architecture**\n",
    "\n",
    "Implement the Transformer's decoder-only structure.\n",
    "This includes\n",
    "\n",
    "* input token embeddings\n",
    "* the causal multi-head self-attention mechanism\n",
    "* feed-forward neural networks\n",
    "* positional encodings, residual connections, layer normalizations.\n",
    "\n",
    "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
    "\n",
    "The `forward` method for the entire model has the following form:\n",
    "\n",
    "```\n",
    "tok_emb = WTE(idx) # token embeddings\n",
    "pos_emb = WPE(pos) # position embeddings\n",
    "x = Dropout(tok_emb + pos_emb)\n",
    "for Block in Blocks:\n",
    "    x = Block(x)\n",
    "x = Final_LayerNorm(x)\n",
    "logits = LM_Head(x)\n",
    "```\n",
    "\n",
    "The `forward` method for the transformer block has the following form:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
    "out = x + self.MLP(self.LayerNorm_2(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Training**\n",
    "\n",
    "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
    "\n",
    "Preprocess the dataset to a character-level representation.\n",
    "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
    "Implement causal masking for the self-attention mechanism.\n",
    "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
    "\n",
    "**Optional**:\n",
    "\n",
    "* Implement a learning rate decay strategy\n",
    "* Implement gradient clipping\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **Evaluation and Inference**\n",
    "\n",
    "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
    "\n",
    "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
    "\n",
    "The high-level pseudocode for generation is:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = \"O God, O God!\"\n",
    "    tokenized_context = tokenize(context)\n",
    "    # the model should implement a method to generate tokens given a prompt\n",
    "    y = model.generate(tokenized, ...)\n",
    "    completion = tokens_to_string(y)\n",
    "```\n",
    "\n",
    "**Optional**:\n",
    "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t88Dcn8JZ8M"
   },
   "source": [
    "### **Example Outputs**\n",
    "\n",
    "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "O God, O God! neither? unto the base very ears,\n",
    "As damned with it.\n",
    "\n",
    "DUKE OF YORK:\n",
    "Away! Once more, one word.\n",
    "\n",
    "RICHARD:\n",
    "Clove, dear so; and therein my son will be\n",
    "false of woe: if ye seems to be the mother\n",
    "Of gracious order this time when R going kinsperse eyes,\n",
    "What dost bewreck her fairer drying tears.\n",
    "\n",
    "NORTHUMBERLAND:\n",
    "Have you forgot the Duke of Norfolk, get him to\n",
    "again; and and agilic: there is my spirit\n",
    "So maly did must such a marble perfection.\n",
    "\n",
    "ELBOW:\n",
    "Come, bring them with oaths, and so deliver\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0SY7CGAhnkp"
   },
   "source": [
    "### Resources:\n",
    "\n",
    "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZdSRWPmgt-H"
   },
   "source": [
    "## Proposed Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the CharDataset dataset, which handles tokenization and provides batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "        self.data = [self.stoi[ch] for ch in data]\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx+self.block_size+1]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work right i need the self-attention-mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, block_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)\n",
    "        self.proj = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        attn_mask = self.mask[:T, :T].to(x.device)\n",
    "        x = x.transpose(0, 1)  # (T, B, C)\n",
    "        x, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = x.transpose(0, 1)  # (B, T, C)\n",
    "        return self.dropout(self.proj(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, ff_hidden_size, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "        self.attn = CausalSelfAttention(embed_size, num_heads, block_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, ff_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_size, embed_size),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Transformer model with a GPT structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embed_size, num_heads, num_layers, ff_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, embed_size))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads, ff_hidden_size, block_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = self.tok_emb(idx)\n",
    "        position_embeddings = self.pos_emb[:, :T, :]\n",
    "        x = token_embeddings + position_embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.pos_emb.size(1):]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 64\n",
    "embed_size = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "ff_hidden_size = 1024\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "epochs = 50\n",
    "\n",
    "# Data Loading\n",
    "with open(\"sonnets.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "dataset = CharDataset(data, block_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Modello\n",
    "model = GPT(\n",
    "    vocab_size=dataset.get_vocab_size(),\n",
    "    block_size=block_size,\n",
    "    embed_size=embed_size,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    ff_hidden_size=ff_hidden_size,\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    # Save a checkpoint every few epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f\"shakespeare_gpt_epoch{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "# Salva il modello\n",
    "torch.save(model.state_dict(), \"shakespeare_gpt.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1, Loss: 0.0483\n",
    "Epoch 2, Loss: 0.0264\n",
    "Epoch 3, Loss: 0.0216\n",
    "Epoch 4, Loss: 0.0274\n",
    "Epoch 5, Loss: 0.0243\n",
    "Epoch 6, Loss: 0.0142\n",
    "Epoch 7, Loss: 0.0088\n",
    "Epoch 8, Loss: 0.0418\n",
    "Epoch 9, Loss: 0.0659\n",
    "Epoch 10, Loss: 0.0112\n",
    "Epoch 11, Loss: 0.0228\n",
    "Epoch 12, Loss: 0.0061\n",
    "Epoch 13, Loss: 0.0022\n",
    "Epoch 14, Loss: 0.0538\n",
    "Epoch 15, Loss: 0.0027\n",
    "Epoch 16, Loss: 0.0571\n",
    "Epoch 17, Loss: 0.0010\n",
    "Epoch 18, Loss: 0.0012\n",
    "Epoch 19, Loss: 0.0466\n",
    "Epoch 20, Loss: 0.0052\n",
    "Epoch 21, Loss: 0.0002\n",
    "Epoch 22, Loss: 0.0676\n",
    "Epoch 23, Loss: 0.0009\n",
    "Epoch 24, Loss: 0.0552\n",
    "Epoch 25, Loss: 0.0151\n",
    "Epoch 26, Loss: 0.0307\n",
    "Epoch 27, Loss: 0.0508\n",
    "Epoch 28, Loss: 0.0260\n",
    "Epoch 29, Loss: 0.0242\n",
    "Epoch 30, Loss: 0.0093\n",
    "Epoch 31, Loss: 0.0456\n",
    "Epoch 32, Loss: 0.0003\n",
    "Epoch 33, Loss: 0.0433\n",
    "Epoch 34, Loss: 0.0072\n",
    "Epoch 35, Loss: 0.0040\n",
    "Epoch 36, Loss: 0.0310\n",
    "Epoch 37, Loss: 0.0144\n",
    "Epoch 38, Loss: 0.0012\n",
    "Epoch 39, Loss: 0.0319\n",
    "Epoch 40, Loss: 0.0182\n",
    "Epoch 41, Loss: 0.0123\n",
    "Epoch 42, Loss: 0.0024\n",
    "Epoch 43, Loss: 0.0441\n",
    "Epoch 44, Loss: 0.0585\n",
    "Epoch 45, Loss: 0.0552\n",
    "Epoch 46, Loss: 0.0502\n",
    "Epoch 47, Loss: 0.0652\n",
    "Epoch 48, Loss: 0.0548\n",
    "Epoch 49, Loss: 0.0444\n",
    "Epoch 50, Loss: 0.0402"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "context = \"O God, O God!\"\n",
    "idx = torch.tensor([dataset.stoi[ch] for ch in context], dtype=torch.long).unsqueeze(0).to(\"cuda\")\n",
    "generated = model.generate(idx, max_new_tokens=600)\n",
    "print(\"\".join([dataset.itos[i] for i in generated[0].tolist()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O God, O God! OOm O O\n",
    "Om OOm OOm OOm OOm OOm OOm OOm OOm Om Om Onloud that his rarfl rank,\n",
    "From find looks my appeachered shakes still.\n",
    "Therefore my love, so precious his lie,\n",
    "And almost tashion the face as shall showers,\n",
    "Who alter than put'st doth sible doth stay?\n",
    "And how I whose grief in goodlen of Praise,\n",
    "Even pain on happy at all thy smight would bar,\n",
    "Whose hadow my prise, and all wirest spect,\n",
    "That then famiend beauty, and watery the pach\n",
    "Our height him habiet his more will be rhymn,\n",
    "Of me of his leases is just-be blest,\n",
    "Shilives mauding his misuse antique char,\n",
    "And he barth your war night beside hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = \"\".join([dataset.itos[i] for i in generated[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated text to a file\n",
    "output_file = \"shakespeare_generated.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(generated_text)\n",
    "\n",
    "print(f\"Generated text saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low (1–10):\n",
    "Indicates the model is highly accurate in predicting the next character.\n",
    "A perplexity close to 1 means the model assigns very high probabilities to the correct sequence, showing excellent generalization.\n",
    "\n",
    "Intermediate (10–100):\n",
    "This is the typical range for a well-trained model on a complex dataset like Shakespeare's works.\n",
    "Perplexity values around 20–50 are considered good for natural text generation tasks, but this can vary depending on the level of granularity (character-level, word-level) and sequence length.\n",
    "\n",
    "High (>100):\n",
    "Indicates the model struggles to capture dependencies in the dataset.\n",
    "This could be due to undertraining, insufficient architecture (e.g., too few layers or small embedding dimensions), or an especially complex dataset.\n",
    "\n",
    "Very High (>>1000):\n",
    "Extremely high perplexity generally suggests the model is failing to learn the structure of the text, assigning nearly uniform probabilities to different options.\n",
    "This is common in the early stages of training or when there's an issue with the data or model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_perplexity(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # Sum to compute total log-likelihood\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)  # (B, T, vocab_size)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_count += y.numel()\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "# Compute perplexity on validation set\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)  # Use a validation split\n",
    "perplexity = compute_perplexity(model, val_dataloader, device=\"cuda\")\n",
    "print(f\"Validation Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Perplexity: 10.01"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
